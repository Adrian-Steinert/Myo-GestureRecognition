{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from glob import glob\n",
    "from gesture_comparison_helper import acc_print, emg_print, gyro_print, orientation_print\n",
    "from gesture_comparison_helper import orientation_euler_print, all_print, load_gestures\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (30.0, 30.0)\n",
    "font = {\n",
    "    'weight': 'bold',\n",
    "    'size': 25\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "def gesture_start_end(gesture_dict):\n",
    "    acc_timestamp_start = gesture_dict['accelerometer']['timestamps'][0]\n",
    "    acc_timestamp_end = gesture_dict['accelerometer']['timestamps'][-1]\n",
    "    emg_timestamp_start = gesture_dict['emg']['timestamps'][0]\n",
    "    emg_timestamp_end = gesture_dict['emg']['timestamps'][-1]\n",
    "    gyro_timestamp_start = gesture_dict['gyro']['timestamps'][0]\n",
    "    gyro_timestamp_end = gesture_dict['gyro']['timestamps'][-1]\n",
    "    ori_timestamp_start = gesture_dict['orientation']['timestamps'][0]\n",
    "    ori_timestamp_end = gesture_dict['orientation']['timestamps'][-1]\n",
    "    \n",
    "    #if acc_timestamp_start - gyro_timestamp_start != 0 or acc_timestamp_start - ori_timestamp_start != 0 or acc_timestamp_end - gyro_timestamp_end != 0 or acc_timestamp_end - ori_timestamp_end != 0:\n",
    "    if len(gesture_dict['accelerometer']['timestamps']) == 10 or len(gesture_dict['gyro']['timestamps']) == 10 or len(gesture_dict['orientation']['timestamps']) == 10:\n",
    "        print('User: {}\\nGesture: {}\\nTimestamp: {}'.format(gesture_dict['performed_by'], gesture_dict['gesture'], gesture_dict['datetime']))\n",
    "        print('-'*25)\n",
    "        print('Starts:')\n",
    "        print('acc-emg: {}'.format(acc_timestamp_start - emg_timestamp_start))\n",
    "        #print('acc-gyro: {}'.format(acc_timestamp_start - gyro_timestamp_start))\n",
    "        #print('acc-ori: {}'.format(acc_timestamp_start - ori_timestamp_start))\n",
    "        print('-'*25)\n",
    "        print('Ends:')\n",
    "        print('acc-emg: {}'.format(acc_timestamp_end - emg_timestamp_end))\n",
    "        #print('acc-gyro: {}'.format(acc_timestamp_end - gyro_timestamp_end))\n",
    "        #print('acc-ori: {}'.format(acc_timestamp_end - ori_timestamp_end))\n",
    "        print()\n",
    "\n",
    "def calculate_rms(value_list):\n",
    "    value_list = np.array(value_list)\n",
    "    return np.sqrt(np.mean(np.square(value_list)))\n",
    "\n",
    "test_dict = load_gestures(glob('../Data/converted/adrian/big_stop/*'))\n",
    "test_dict += load_gestures(glob('../Data/converted/megan/big_stop/*'))\n",
    "#test_dict = load_gestures(glob('../Data/converted/*/*/*'))\n",
    "\n",
    "#print(calculate_rms(test_dict[0]['emg']['1']))\n",
    "\n",
    "emg_len = []\n",
    "imu_len = []\n",
    "\n",
    "for e in test_dict:\n",
    "    #gesture_start_end(e)\n",
    "    emg_len.append(len(e['emg']['timestamps']))\n",
    "    imu_len.append(len(e['accelerometer']['timestamps']))\n",
    "    imu_len.append(len(e['gyro']['timestamps']))\n",
    "    imu_len.append(len(e['orientation']['timestamps']))\n",
    "    [print('RMS for EMG {}: {}'.format(str(x), calculate_rms(e['emg'][str(x)]))) for x in range(1, 9, 1)]\n",
    "    print()\n",
    "\n",
    "print('\\nMean EMG length: {}'.format(np.mean(emg_len)))\n",
    "print('Mean IMU length: {}'.format(np.mean(imu_len)))\n",
    "\n",
    "#emg_len.remove(20)\n",
    "#imu_len.remove(5)\n",
    "#imu_len.remove(5)\n",
    "#imu_len.remove(5)\n",
    "\n",
    "#emg_len.remove(40)\n",
    "#imu_len.remove(10)\n",
    "#imu_len.remove(10)\n",
    "#imu_len.remove(10)\n",
    "\n",
    "#print('Min EMG length: {}'.format(min(emg_len)))\n",
    "#print('Max EMG length: {}'.format(max(emg_len)))\n",
    "#print('Min IMU length: {}'.format(min(imu_len)))\n",
    "#print('Max IMU length: {}'.format(max(imu_len)))\n",
    "   \n",
    "#print(sorted(emg_len))\n",
    "#print(sorted(imu_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "test_dict = load_gestures(glob('../Data/converted/adrian/big_stop/big_stop2017-07-17_22-11-56.p'))\n",
    "test_dict = test_dict[0]\n",
    "\n",
    "acc_print([test_dict])\n",
    "#print(type(test_dict))\n",
    "acc_x = test_dict['accelerometer']['x']\n",
    "acc_y = test_dict['accelerometer']['y']\n",
    "acc_z = test_dict['accelerometer']['z']\n",
    "#print(acc_x)\n",
    "\n",
    "acc_x = np.array(acc_x)\n",
    "acc_shape = acc_x.shape\n",
    "acc_y = np.array(acc_y)\n",
    "acc_z = np.array(acc_z)\n",
    "\n",
    "\n",
    "scaled_value_x = scaler.fit_transform(np.array(acc_x).reshape(-1, 1)).reshape(acc_shape)\n",
    "scaled_value_y = scaler.fit_transform(np.array(acc_y).reshape(-1, 1)).reshape(acc_shape)\n",
    "scaled_value_z = scaler.fit_transform(np.array(acc_z).reshape(-1, 1)).reshape(acc_shape)\n",
    "\n",
    "#test_dict['accelerometer']['x'] = scaled_value_x\n",
    "#test_dict['accelerometer']['y'] = scaled_value_y\n",
    "#test_dict['accelerometer']['z'] = scaled_value_z\n",
    "\n",
    "acc_print([test_dict])\n",
    "\n",
    "\n",
    "\n",
    "#print('EMG  length: {}'.format(len(test_dict['emg']['1'])))\n",
    "print('ACC  length: {}'.format(len(test_dict['accelerometer']['x'])))\n",
    "#print('GYRO length: {}'.format(len(test_dict['gyro']['x'])))\n",
    "#print('ORI  length: {}'.format(len(test_dict['orientation']['x'])))\n",
    "#print()\n",
    "#print('EMG  start: {}'.format(test_dict['emg']['timestamps'][1]))\n",
    "print('ACC  start: {}'.format(test_dict['accelerometer']['timestamps'][0]))\n",
    "#print('GYRO start: {}'.format(test_dict['gyro']['timestamps'][0]))\n",
    "#print('ORI  start: {}'.format(test_dict['orientation']['timestamps'][0]))\n",
    "#print()\n",
    "print('ACC  end: {}'.format(test_dict['accelerometer']['timestamps'][-1]))\n",
    "#print('GYRO end: {}'.format(test_dict['gyro']['timestamps'][-1]))\n",
    "#print('ORI  end: {}'.format(test_dict['orientation']['timestamps'][-1]))\n",
    "#print('EMG  end: {}'.format(test_dict['emg']['timestamps'][-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "acc_train = load_gestures(glob('../Data/converted/adrian/big_stop/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/adrian/come_up/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/adrian/come_here/*'))\n",
    "\n",
    "\n",
    "acc_test = load_gestures(glob('../Data/converted/ruben/big_stop/*'))\n",
    "acc_test += load_gestures(glob('../Data/converted/ruben/come_up/*'))\n",
    "acc_test += load_gestures(glob('../Data/converted/ruben/come_here/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_train = load_gestures(glob('../Data/converted/a*/*/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/cynthia/*/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/flo/*/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/kai/*/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/lisa/*/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/m*/*/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/nina_v/*/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/oli/*/*'))\n",
    "acc_train += load_gestures(glob('../Data/converted/paul/*/*'))\n",
    "\n",
    "acc_test = load_gestures(glob('../Data/converted/ruben/*/*'))\n",
    "acc_test += load_gestures(glob('../Data/converted/s*/*/*'))\n",
    "\n",
    "\n",
    "FRAME_NUMBER = 9\n",
    "\n",
    "classifier = svm.SVC(kernel='linear',\n",
    "                     decision_function_shape='ovo')\n",
    "\n",
    "def prepare_data(acc_train, for_training=False):\n",
    "    label = []\n",
    "    train_input = []\n",
    "    features_per_gesture = []\n",
    "    \n",
    "    for gesture in acc_train:\n",
    "        sensor_data = [gesture['accelerometer']['x'], gesture['accelerometer']['y'], gesture['accelerometer']['z']]\n",
    "        splitted_frames = split_in_frames(sensor_data, FRAME_NUMBER)\n",
    "\n",
    "        if splitted_frames is None:\n",
    "            data_length = len(gesture['accelerometer']['timestamps'])\n",
    "            print('split_in_frames() returned None: Data length was {} but needs to be bigger than {} + 1'.format(data_length\n",
    "                                                                                                                 , FRAME_NUMBER))\n",
    "            continue\n",
    "\n",
    "        features = create_acc_feature_vector(splitted_frames)\n",
    "\n",
    "        if features is not None:\n",
    "            features_per_gesture.append(features)\n",
    "            label.append(gesture['gesture'])\n",
    "\n",
    "    for feature_list in features_per_gesture:\n",
    "        features = np.array(feature_list)\n",
    "        o_shape = features.shape\n",
    "        #if True in np.isnan(features):\n",
    "            #print(features_per_gesture.index(feature_list))\n",
    "            #print(feature_list)\n",
    "        #features = scaler.fit_transform(features.reshape(-1, 1)).reshape(o_shape)\n",
    "\n",
    "        train_input.append(features)\n",
    "    if for_training:\n",
    "        return train_input, label\n",
    "    else:\n",
    "        return train_input\n",
    "\n",
    "train_input, label = prepare_data(acc_train, for_training=True)\n",
    "test_input = prepare_data(acc_train, for_training=False)\n",
    "\n",
    "#print(test_input)\n",
    "\n",
    "#classifier.fit(train_input, label)\n",
    "#print(label)\n",
    "#predictions = classifier.predict(test_input)\n",
    "#print(predictions)\n",
    "#print(label == predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "Y = ['None', 'gesture', 'whatever']\n",
    "\n",
    "clf.fit(X, Y)\n",
    "clf.predict([[0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = 123\n",
    "frames = 9\n",
    "# every two adjunct sequences make a frame thus input has one sequence more than frames\n",
    "sequence_count = frames + 1\n",
    "segment_length = length // sequence_count\n",
    "cut_off = length % sequence_count\n",
    "frame_length = 2 * segment_length\n",
    "print('Segment length: {}'.format(segment_length))\n",
    "\n",
    "data = list(range(length))\n",
    "#print(data)\n",
    "data = data[:-cut_off]\n",
    "print()\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "segmented_data = [data[segment:segment + frame_length] for segment in range(0, len(data), segment_length)]\n",
    "print(segmented_data)\n",
    "#print(type(np.array(segmented_data)[0]))\n",
    "#print(np.array(segmented_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_in_frames(sensor_data, frame_number):\n",
    "    '''\n",
    "    every two adjunct sequences make a frame\n",
    "    => input data has one segment more than frames\n",
    "    => frame length is two times segment lenght\n",
    "    '''\n",
    "    data_length = len(sensor_data[0])\n",
    "    segment_count = frame_number + 1\n",
    "    segment_length = data_length // segment_count\n",
    "    \n",
    "    # check if data is long enough for given frame_number\n",
    "    if segment_length <= 0:\n",
    "        return None\n",
    "    \n",
    "    cut_off = data_length % segment_count\n",
    "    frame_length = 2 * segment_length\n",
    "\n",
    "    segmented_data = []\n",
    "    for sensor_axis in sensor_data:\n",
    "        if cut_off > 0:\n",
    "            filtered_sensor_axis = sensor_axis[:-cut_off]\n",
    "        else:\n",
    "            filtered_sensor_axis = sensor_axis[:]\n",
    "\n",
    "        segmented_axis = []\n",
    "        for segment in range(0, len(filtered_sensor_axis), segment_length):\n",
    "            step_size = segment + frame_length\n",
    "            segmented_axis.append(filtered_sensor_axis[segment:step_size])\n",
    "\n",
    "        segmented_data.append(segmented_axis)\n",
    "\n",
    "    return segmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitted = split_in_frames(data, 9)\n",
    "print(splitted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "transformed = np.fft.fft(splitted[0])\n",
    "\n",
    "\n",
    "mean = transformed[0]\n",
    "print('Mean: {}'.format(mean))\n",
    "energy = np.sum(np.square(np.absolute(transformed[1:]))) / len(transformed)\n",
    "print('Energy: {}'.format(energy))\n",
    "probability = np.divide(np.absolute(transformed), np.sum(np.absolute(transformed[1:])))\n",
    "entropy = np.sum(np.multiply(probability, np.log2(np.divide(1, probability))))\n",
    "print('Entropy: {}'.format(entropy))\n",
    "print('\\nDuration: {}'.format(time.clock()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "transformed = fft(splitted[0])\n",
    "\n",
    "\n",
    "mean = transformed[0]\n",
    "print('Mean: {}'.format(mean))\n",
    "energy = np.divide(np.sum(np.square(np.absolute(transformed[1:]))), len(transformed))\n",
    "print('Energy: {}'.format(energy))\n",
    "probability = np.divide(np.absolute(transformed), np.sum(np.absolute(transformed[1:])))\n",
    "entropy = np.sum(np.multiply(probability, np.log2(np.divide(1, probability))))\n",
    "print('Entropy: {}'.format(entropy))\n",
    "std_dev = np.std(splitted[0])\n",
    "print('Standard deviation: {}'.format(std_dev))\n",
    "std_dev = np.std(splitted[0])\n",
    "print('Correlation x-y: {}'.format(std_dev))\n",
    "print('\\nDuration: {}'.format(time.clock()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_value = np.divide(splitted[0], np.sum(splitted[0]))\n",
    "#mean_value = np.average(splitted[0])\n",
    "#print(splitted[0])\n",
    "#print(np.sum(splitted[0]))\n",
    "#print(mean_value)\n",
    "\n",
    "standard_deviation = np.sqrt(np.sum(np.square(np.subtract(splitted[0], mean_value))))\n",
    "print(standard_deviation)\n",
    "#print(np.var(splitted[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal.correlate(splitted[0], splitted[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fragment_x = test_dict['accelerometer']['x'][:24]\n",
    "fragment_y = test_dict['accelerometer']['y'][:24]\n",
    "fragment_z = test_dict['accelerometer']['z'][:24]\n",
    "\n",
    "#print(fragment_x)\n",
    "#print(fragment_y)\n",
    "\n",
    "axis_1 = fragment_y\n",
    "axis_2 = fragment_z\n",
    "\n",
    "different_axis = np.divide(np.sum(np.abs(np.multiply(axis_1, axis_2))), len(axis_1))\n",
    "same_axis_0 = np.divide(np.sum(np.abs(np.multiply(axis_1, axis_1))), len(axis_1))\n",
    "same_axis_1 = np.divide(np.sum(np.abs(np.multiply(axis_2, axis_2))), len(axis_1))\n",
    "#print(different_axis)\n",
    "#print(same_axis_0)\n",
    "#print(same_axis_1)\n",
    "#print()\n",
    "\n",
    "correlation_enumerator = np.subtract(different_axis, np.multiply(np.mean(axis_1), np.mean(axis_2)))\n",
    "correlation_denominator = np.multiply(np.sqrt(np.subtract(same_axis_0, np.square(np.mean(axis_1)))), np.sqrt(np.subtract(same_axis_1, np.square(np.mean(axis_2)))))\n",
    "correlation = np.divide(correlation_enumerator, correlation_denominator)\n",
    "#print(correlation_enumerator)\n",
    "#print(correlation_denominator)\n",
    "\n",
    "#print(np.mean(splitted[0]))\n",
    "#print(np.mean(splitted[4]))\n",
    "#print(np.mean(splitted[0])**2)\n",
    "#print(np.mean(splitted[4])**2)\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "st.pearsonr(fragment_x, fragment_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_test = [[[1,2], [2,3]], [[4, 5], [5, 6]]]\n",
    "\n",
    "for x in zip([[[1,2], [2,3]], [[4, 5], [5, 6]]]):\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_acc_feature_vector(frame_list):\n",
    "    if frame_list is None:\n",
    "        return None\n",
    "    \n",
    "    acc_feature_vector = []\n",
    "    x_axis = frame_list[0]\n",
    "    y_axis = frame_list[1]\n",
    "    z_axis = frame_list[2]\n",
    "\n",
    "    for x_segment, y_segment, z_segment in zip(x_axis, y_axis, z_axis):\n",
    "        acc_feature_vector.extend(create_frequency_domain_features(x_segment, y_segment, z_segment))\n",
    "        acc_feature_vector.extend(create_time_domain_features(x_segment, y_segment, z_segment))\n",
    "\n",
    "    return acc_feature_vector\n",
    "\n",
    "\n",
    "def create_frequency_domain_features(x_segment, y_segment, z_segment):\n",
    "    frequency_domain_features = []\n",
    "\n",
    "    # first feature: mean\n",
    "    x_fft = fft(x_segment)\n",
    "    y_fft = fft(y_segment)\n",
    "    z_fft = fft(z_segment)\n",
    "    x_mean = x_fft[0]\n",
    "    y_mean = y_fft[0]\n",
    "    z_mean = z_fft[0]\n",
    "    frequency_domain_features.extend([x_mean, y_mean, z_mean])\n",
    "\n",
    "    # second feature: energy\n",
    "    x_energy = calculate_energy(x_fft[1:])\n",
    "    y_energy = calculate_energy(y_fft[1:])\n",
    "    z_energy = calculate_energy(z_fft[1:])\n",
    "    frequency_domain_features.extend([x_energy, y_energy, z_energy])\n",
    "\n",
    "    # third feature: entropy\n",
    "    x_entropy = calculate_entropy(x_fft, x_fft[1:])\n",
    "    y_entropy = calculate_entropy(y_fft, y_fft[1:])\n",
    "    z_entropy = calculate_entropy(z_fft, z_fft[1:])\n",
    "    frequency_domain_features.extend([x_entropy, y_entropy, z_entropy])\n",
    "\n",
    "    return frequency_domain_features\n",
    "\n",
    "\n",
    "def calculate_energy(fft_no_mean_segment):\n",
    "    energy = np.divide(np.sum(np.square(np.absolute(fft_no_mean_segment))), len(fft_no_mean_segment + 1))\n",
    "    if math.isnan(energy):\n",
    "        print('energy')\n",
    "        print('enumerator: {}'.format(np.sum(np.square(np.absolute(fft_no_mean_segment)))))\n",
    "        print('denominator: {}'.format(len(fft_no_mean_segment + 1)))\n",
    "    return energy\n",
    "\n",
    "\n",
    "def calculate_entropy(fft_segment, fft_no_mean_segment):\n",
    "    probability = np.divide(np.absolute(fft_segment), np.sum(np.absolute(fft_no_mean_segment)))\n",
    "    entropy = np.sum(np.multiply(probability, np.log2(np.divide(1, probability))))\n",
    "    if math.isnan(entropy):\n",
    "        print('entropy')\n",
    "        print('probability-denominator: {}'.format(np.sum(np.absolute(fft_no_mean_segment))))\n",
    "        print('probability: {}'.format(probability))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def create_time_domain_features(x_segment, y_segment, z_segment):\n",
    "    time_domain_features = []\n",
    "\n",
    "    # fourth feature: standard deviation\n",
    "    x_std_dev = np.std(x_segment)\n",
    "    y_std_dev = np.std(y_segment)\n",
    "    z_std_dev = np.std(z_segment)\n",
    "    time_domain_features.extend([x_std_dev, y_std_dev, z_std_dev])\n",
    "    \n",
    "    # fith feature: axis correlation\n",
    "    x_y_correlation = calculate_axis_correlation(x_segment, y_segment)\n",
    "    x_z_correlation = calculate_axis_correlation(x_segment, z_segment)\n",
    "    y_z_correlation = calculate_axis_correlation(y_segment, z_segment)\n",
    "    time_domain_features.extend([x_y_correlation, x_z_correlation, y_z_correlation])\n",
    "\n",
    "    return time_domain_features\n",
    "\n",
    "\n",
    "def calculate_axis_correlation(axis_1, axis_2):\n",
    "    different_axis = np.divide(np.sum(np.abs(np.multiply(axis_1, axis_2))), len(axis_1))\n",
    "    same_axis_1 = np.divide(np.sum(np.abs(np.multiply(axis_1, axis_1))), len(axis_1))\n",
    "    same_axis_2 = np.divide(np.sum(np.abs(np.multiply(axis_2, axis_2))), len(axis_1))\n",
    "\n",
    "    correlation_enumerator = np.subtract(different_axis, np.multiply(np.mean(axis_1), np.mean(axis_2)))\n",
    "    correlation_denominator = np.multiply(np.sqrt(np.subtract(same_axis_1, np.square(np.mean(axis_1)))),\n",
    "                                          np.sqrt(np.subtract(same_axis_2, np.square(np.mean(axis_2)))))\n",
    "    correlation = np.divide(correlation_enumerator, correlation_denominator)\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "y = [[['x1', 'x1', 'x1', 'x1', 'x1'],['x2', 'x2', 'x2', 'x2', 'x2']], [['y1', 'y1', 'y1', 'y1', 'y1'],\n",
    "        ['y2', 'y2', 'y2', 'y2', 'y2']] , [['z1', 'z1', 'z1', 'z1', 'z1'],['z2', 'z2', 'z2', 'z2', 'z2']]]\n",
    "x = np.array(y)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "axis_count = len(x)\n",
    "print()\n",
    "\n",
    "#print(type(x))\n",
    "#print(type(x[0]))\n",
    "#print(type(x[0][0]))\n",
    "#print(type(x[0][0][0]))\n",
    "\n",
    "x = x.reshape(-1, x.shape[2], order='F')\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "for axis_index in range(0, len(x), axis_count):\n",
    "    print(x[axis_index:axis_index + axis_count])\n",
    "    print()\n",
    "    for sliced in combinations(x[axis_index:axis_index + axis_count], 2):\n",
    "        print(sliced[0], sliced[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2]\n",
    "a.extend([[3, 4], [5, 6]])\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
